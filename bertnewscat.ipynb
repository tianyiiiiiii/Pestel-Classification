{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "import transformers\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler,TensorDataset\n",
    "# from datasets import Dataset\n",
    "from torch.utils.data import Dataset \n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from transformers import get_scheduler\n",
    "from tqdm.auto import tqdm\n",
    "import evaluate\n",
    "from torch.nn import CrossEntropyLoss\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>link</th>\n",
       "      <th>headline</th>\n",
       "      <th>category</th>\n",
       "      <th>short_description</th>\n",
       "      <th>authors</th>\n",
       "      <th>date</th>\n",
       "      <th>PESTEL_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.huffingtonpost.com/entry/mortgage-...</td>\n",
       "      <td>Mortgage Deal Reached In 2008 Shows Pitfalls T...</td>\n",
       "      <td>BUSINESS</td>\n",
       "      <td>The Obama administration, which is pushing sta...</td>\n",
       "      <td>Loren Berlin</td>\n",
       "      <td>2012-02-05</td>\n",
       "      <td>Economic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.huffingtonpost.com/entry/women-in-...</td>\n",
       "      <td>Women in Business: Kate O'Brien Minson, Presid...</td>\n",
       "      <td>BUSINESS</td>\n",
       "      <td>Kate has lived and breathed the therapeutic ap...</td>\n",
       "      <td>Laura Dunn, ContributorSocial Media and Commun...</td>\n",
       "      <td>2015-04-25</td>\n",
       "      <td>Economic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.huffingtonpost.com/entry/like-athl...</td>\n",
       "      <td>Like Athletes, Business Owners Need to Learn F...</td>\n",
       "      <td>BUSINESS</td>\n",
       "      <td>Business owners and top executives can also \"w...</td>\n",
       "      <td>Mary Ellen Biery, ContributorResearch Speciali...</td>\n",
       "      <td>2015-01-19</td>\n",
       "      <td>Economic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.huffingtonpost.com/entry/donald-tr...</td>\n",
       "      <td>Trump Could Trigger The Longest Recession Sinc...</td>\n",
       "      <td>BUSINESS</td>\n",
       "      <td>Yikes.</td>\n",
       "      <td>Ben Walsh</td>\n",
       "      <td>2016-06-27</td>\n",
       "      <td>Economic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.huffingtonpost.com/entry/grocery-c...</td>\n",
       "      <td>Grocery Chains Made A Promise To The First Lad...</td>\n",
       "      <td>BUSINESS</td>\n",
       "      <td>An AP investigation found that major grocers o...</td>\n",
       "      <td>Mike Schneider, AP</td>\n",
       "      <td>2015-12-07</td>\n",
       "      <td>Economic</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                link  \\\n",
       "0  https://www.huffingtonpost.com/entry/mortgage-...   \n",
       "1  https://www.huffingtonpost.com/entry/women-in-...   \n",
       "2  https://www.huffingtonpost.com/entry/like-athl...   \n",
       "3  https://www.huffingtonpost.com/entry/donald-tr...   \n",
       "4  https://www.huffingtonpost.com/entry/grocery-c...   \n",
       "\n",
       "                                            headline  category  \\\n",
       "0  Mortgage Deal Reached In 2008 Shows Pitfalls T...  BUSINESS   \n",
       "1  Women in Business: Kate O'Brien Minson, Presid...  BUSINESS   \n",
       "2  Like Athletes, Business Owners Need to Learn F...  BUSINESS   \n",
       "3  Trump Could Trigger The Longest Recession Sinc...  BUSINESS   \n",
       "4  Grocery Chains Made A Promise To The First Lad...  BUSINESS   \n",
       "\n",
       "                                   short_description  \\\n",
       "0  The Obama administration, which is pushing sta...   \n",
       "1  Kate has lived and breathed the therapeutic ap...   \n",
       "2  Business owners and top executives can also \"w...   \n",
       "3                                             Yikes.   \n",
       "4  An AP investigation found that major grocers o...   \n",
       "\n",
       "                                             authors        date PESTEL_label  \n",
       "0                                       Loren Berlin  2012-02-05     Economic  \n",
       "1  Laura Dunn, ContributorSocial Media and Commun...  2015-04-25     Economic  \n",
       "2  Mary Ellen Biery, ContributorResearch Speciali...  2015-01-19     Economic  \n",
       "3                                          Ben Walsh  2016-06-27     Economic  \n",
       "4                                 Mike Schneider, AP  2015-12-07     Economic  "
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/balancednewcategory.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PESTEL_label</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Economic</td>\n",
       "      <td>Mortgage Deal Reached In 2008 Shows Pitfalls T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Economic</td>\n",
       "      <td>Women in Business: Kate O'Brien Minson, Presid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Economic</td>\n",
       "      <td>Like Athletes, Business Owners Need to Learn F...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Economic</td>\n",
       "      <td>Trump Could Trigger The Longest Recession Sinc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Economic</td>\n",
       "      <td>Grocery Chains Made A Promise To The First Lad...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  PESTEL_label                                            content\n",
       "0     Economic  Mortgage Deal Reached In 2008 Shows Pitfalls T...\n",
       "1     Economic  Women in Business: Kate O'Brien Minson, Presid...\n",
       "2     Economic  Like Athletes, Business Owners Need to Learn F...\n",
       "3     Economic  Trump Could Trigger The Longest Recession Sinc...\n",
       "4     Economic  Grocery Chains Made A Promise To The First Lad..."
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"content\"] = df[\"headline\"] + \" \" + df[\"short_description\"]\n",
    "df = df[['PESTEL_label', 'content']]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert content to string and handle 'NaN' values\n",
    "df['content'] = df['content'].apply(lambda x: '' if pd.isna(x) else str(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = text.replace('\\n', ' ').strip()\n",
    "    return text\n",
    "\n",
    "df['content'] = df['content'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(df, test_size=0.2, stratify=df['PESTEL_label'], random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewsDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_len, pestel_to_idx):\n",
    "        self.df = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.pestel_to_idx = pestel_to_idx\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self.df.iloc[index] \n",
    "        content = row['content']\n",
    "        label = row['PESTEL_label']\n",
    "\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            content,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            return_token_type_ids=True,\n",
    "            truncation=True\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'ids': torch.tensor(inputs['input_ids'], dtype=torch.long),\n",
    "            'mask': torch.tensor(inputs['attention_mask'], dtype=torch.long),\n",
    "            'targets': torch.tensor(self.pestel_to_idx[label], dtype=torch.long)\n",
    "        }\n",
    "        \n",
    "pestel_to_idx = {\n",
    "    \"Political\": 0,\n",
    "    \"Economic\": 1,\n",
    "    \"Social\": 2,\n",
    "    \"Technological\": 3,\n",
    "    \"Environmental\": 4,\n",
    "    \"Legal\": 5\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = transformers.DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "MAX_LEN = 256 # can try 128\n",
    "BATCH_SIZE = 16 # can try 32 / 64\n",
    "\n",
    "train_set = NewsDataset(train_df, tokenizer, MAX_LEN, pestel_to_idx)\n",
    "test_set = NewsDataset(test_df, tokenizer, MAX_LEN, pestel_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "distilbert_model = transformers.DistilBertModel.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PestelClassifier(torch.nn.Module):\n",
    "    def __init__(self, distilbert, num_classes):\n",
    "        super(PestelClassifier, self).__init__()\n",
    "        self.distilbert = distilbert\n",
    "        self.dropout = torch.nn.Dropout(0.3)\n",
    "        self.output = torch.nn.Linear(768, num_classes)\n",
    "        \n",
    "        # self.classifier = torch.nn.Sequential(\n",
    "        #     torch.nn.Linear(768, 256),\n",
    "        #     torch.nn.ReLU(),\n",
    "        #     torch.nn.Dropout(0.3),\n",
    "        #     torch.nn.Linear(256, num_classes)\n",
    "        # )\n",
    "\n",
    "\n",
    "    def forward(self, ids, mask):\n",
    "        output = self.distilbert(ids, attention_mask=mask)\n",
    "        output = self.dropout(output[0][:, 0, :])  # CLS token\n",
    "        output = self.output(output)\n",
    "        return output\n",
    "    \n",
    "        # x = self.distilbert(ids, attention_mask=mask).last_hidden_state[:, 0, :]  # CLS\n",
    "        # x = self.classifier(x)\n",
    "        # return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PestelClassifier(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.3, inplace=False)\n",
       "  (output): Linear(in_features=768, out_features=6, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_classes = 6  # For PESTEL\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = PestelClassifier(distilbert_model, num_classes)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "LEARNING_RATE = 1e-7 # can try higher \n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=LEARNING_RATE)\n",
    "loss_function = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Training: 100%|██████████| 840/840 [30:34<00:00,  2.18s/it, loss=1.8] \n",
      "Epoch 1 - Testing: 100%|██████████| 210/210 [01:51<00:00,  1.89it/s, loss=1.79]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 1.7704, Accuracy: 0.2024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Training: 100%|██████████| 840/840 [32:21<00:00,  2.31s/it, loss=1.8]   \n",
      "Epoch 2 - Testing: 100%|██████████| 210/210 [01:46<00:00,  1.98it/s, loss=1.77]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Loss: 1.7249, Accuracy: 0.3402\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Training: 100%|██████████| 840/840 [27:38<00:00,  1.97s/it, loss=1.65]\n",
      "Epoch 3 - Testing: 100%|██████████| 210/210 [01:33<00:00,  2.23it/s, loss=1.75]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Loss: 1.6590, Accuracy: 0.4497\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 - Training: 100%|██████████| 840/840 [25:19<00:00,  1.81s/it, loss=1.76]\n",
      "Epoch 4 - Testing: 100%|██████████| 210/210 [01:33<00:00,  2.25it/s, loss=1.71]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Loss: 1.5577, Accuracy: 0.5179\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 - Training: 100%|██████████| 840/840 [26:59<00:00,  1.93s/it, loss=1.53]\n",
      "Epoch 5 - Testing: 100%|██████████| 210/210 [01:45<00:00,  2.00it/s, loss=1.64]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, Loss: 1.4435, Accuracy: 0.5723\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6 - Training:  53%|█████▎    | 447/840 [16:47<14:45,  2.25s/it, loss=1.24]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[126], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(ids, mask)\n\u001b[1;32m     11\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_function(outputs, targets)\n\u001b[0;32m---> 12\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     15\u001b[0m train_bar\u001b[38;5;241m.\u001b[39mset_postfix(loss\u001b[38;5;241m=\u001b[39mloss\u001b[38;5;241m.\u001b[39mitem())\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    618\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    619\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    624\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    625\u001b[0m     )\n\u001b[0;32m--> 626\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/autograd/graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    824\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    825\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# change to methods and call -> train for train and test loader, eval for val loader\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    train_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1} - Training\")\n",
    "    for batch in train_bar:\n",
    "        ids = batch['ids'].to(device)\n",
    "        mask = batch['mask'].to(device)\n",
    "        targets = batch['targets'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(ids, mask)\n",
    "        loss = loss_function(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    test_bar = tqdm(test_loader, desc=f\"Epoch {epoch+1} - Testing\")\n",
    "    for batch in test_bar:\n",
    "        ids = batch['ids'].to(device)\n",
    "        mask = batch['mask'].to(device)\n",
    "        targets = batch['targets'].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(ids, mask)\n",
    "            loss = loss_function(outputs, targets)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total_correct += (predicted == targets).sum().item()\n",
    "            total_samples += targets.size(0)\n",
    "\n",
    "            test_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "    avg_loss = total_loss / len(test_loader)\n",
    "    accuracy = total_correct / total_samples\n",
    "\n",
    "    print(f\"Epoch: {epoch + 1}, Loss: {avg_loss:.4f}, Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_bert_model(model, data_loader, loss_function, device, set_name=\"Test\"):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    loop = tqdm(data_loader, desc=f\"{set_name} Evaluation\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in loop:\n",
    "            ids = batch['ids'].to(device)\n",
    "            mask = batch['mask'].to(device)\n",
    "            targets = batch['targets'].to(device)\n",
    "\n",
    "            outputs = model(ids, mask)\n",
    "            loss = loss_function(outputs, targets)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            all_preds.extend(predicted.cpu().tolist())\n",
    "            all_targets.extend(targets.cpu().tolist())\n",
    "\n",
    "            loop.set_postfix(loss=loss.item())\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    accuracy = accuracy_score(all_targets, all_preds)\n",
    "    precision = precision_score(all_targets, all_preds, average='macro', zero_division=0)\n",
    "    recall = recall_score(all_targets, all_preds, average='macro', zero_division=0)\n",
    "    f1 = f1_score(all_targets, all_preds, average='macro', zero_division=0)\n",
    "\n",
    "    print(f\"{set_name} | Loss: {avg_loss:.4f} | Acc: {accuracy*100:.2f}% | P: {precision:.4f} | R: {recall:.4f} | F1: {f1:.4f}\")\n",
    "\n",
    "    return avg_loss, accuracy, precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(\"./checkpoints/best_model.pt\", map_location=device))\n",
    "model.to(device)\n",
    "\n",
    "test_loss, test_accuracy = evaluate_bert_model(\n",
    "    model=model,\n",
    "    data_loader=test_loader,\n",
    "    loss_function=loss_function,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "print(f\"\\nFinal Test Accuracy: {test_accuracy * 100:.2f}% | Test Loss: {test_loss:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
